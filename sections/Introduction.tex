\chapter{Introduction}
\label{introduction}
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%  NEW FIGURE ENVIRONMENT   %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\makeatletter
\newenvironment{figurehere}
{\def\@captype{figure}}
{}
\makeatother

\makeatletter
\newenvironment{Figure_modified}{%
\par\addvspace{12pt plus2pt}%
\def\@captype{figure}%
}{%
\par\addvspace{12pt plus2pt}%
}%


% Custom caption setup to ensure it integrates with the caption package and maintains the text size across all parts
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\bfseries\sffamily\footnotesize #1}: \sffamily\footnotesize #2} % Apply bold, sans-serif and footnotesize to label, footnotesize to text
  \ifdim \wd\@tempboxa >\hsize
    {\bfseries\sffamily\footnotesize #1}: \sffamily\footnotesize #2\par % Maintain text size and styling if wrapping
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}% Centering the caption if shorter than line width
  \fi
  \vskip\belowcaptionskip}
\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\renewcommand{\thesubsection}{\thechapter.\arabic{subsection}}

\pdfbookmark[1]{Ghost Section}{ghost-section}

\subsection{A birds'-eye view of Chemical Biology}
\label{Introduction_birds}

Every phenotypic trait in living organisms, shaped by the unceasing constraints of natural selection, ultimately traces back to underlying molecular events. However, as a result of millions of years of evolution, biological systems have become intrinsically complex, which often obscures the link between genotype and phenotype. Indeed, the human body is composed of around 3·10\textsuperscript{13} cells, the fundamental unit of life \cite{hatton_human_2023, sender_revised_2016, bianconi_estimation_2013}. Depending on cell differentiation and environmental factors, the approximately 20,000 protein-coding genes encoded in the human genome are differently expressed to produce proteins, each varying in number from dozens to millions within each cell\cite{pertea_between_2010, beck_quantitative_2011, ezkurdia_multiple_2014, international_human_genome_sequencing_consortium_finishing_2004, ezkurdia_most_2015}. Proteins, subject to post-translational modifications (PTMs) and arranged in independent functional domains, play fundamental roles in biochemical processes, such as catalyzing chemical reactions (e.g. kinases), transducing signals (e.g. insulin) or providing structural support in cells (e.g. actin)\cite{khoury_proteome-wide_2011, walsh_protein_2005}. At a lower scale, small molecules act as substrates, intermediates and products in many of those processes, such as those related to energy transfer (e.g. ATP) and signal transduction (e.g. serotonin). Within every cell, a plethora of reactions and interactions among biochemical entities, such as proteins and small molecules, occur within the realm of stochasticity driven by binding specificity. These events are intricately orchestrated and coordinated to sustain cell growth and survival. A disruption or malfunction in a single component of these processes may lead to the onset of human disease.

\subsection{Early small molecule pharmacology}
\label{Introduction_early}

Traditionally, the discovery of chemical compounds with the capacity to modulate or revert human disease states (i.e. drugs) was mostly serendipitous and entirely based on the therapeutic properties of substances natively present in nature (e.g. natural products) \cite{sneader_drug_2005}. This is the case of morphine \cite{sneader_drug_2005}, a powerful pain reliever alkaloid firstly isolated from opium in the early 19th century, and quinine \cite{achan_quinine_2011}, isolated from cinchona bark a couple of decades later, and used to treat malaria. Although the use of pure natural products further led to outstanding breakthroughs, such as Fleming’s revolutionary penicillin discovery in 1928 \cite{fleming_antibacterial_2001}, the idea that external compounds could be used to treat human diseases led to the development of the first synthetic small molecule drugs at the end of the 19th century (e.g. chloral hydrate and acetylsalicylic acid) \cite{sneader_drug_2005, vane_mechanism_2003}. At that time, a reductionist view of human disease prevailed, epitomized by the concept of the “magic bullet”, a term developed by the Nobel laureate and chemotherapy pioneer Paul Ehrlich in 1907 \cite{strebhardt_paul_2008, schwartz_paul_2004}. Ehrlich envisioned an optimal therapy in which an external agent (i.e. a chemical compound) selectively targeted and killed the disease-causing pathogen (e.g. bacteria) without any further effect in the host organism (i.e. the human body). Indeed, Ehrlich’s laboratory discovered arsphenamine in 1909, the first effective treatment for syphilis, also termed as the first designed magic bullet\cite{bosch_contributions_2008}. However, and unlike natural products, which are commonly characterized by intricate structures and tailored attributes shaped over eons of natural evolution\cite{grigalunas_chemical_2022}, synthetic drugs were (and still are) imperfect human inventions with suboptimal properties and bioactivities.

Despite pharmacological research being conducted from a primarily phenotypical perspective at that time, drug macromolecular counterparts simultaneously began to attract interest as a means to understand general biological processes and, in particular, the action of drugs. Paul Enrich and John N. Langley introduced the concept of receptors and laid the foundation for modern pharmacology at the beginning of the 20th century. Additionally, the concept of enzyme inhibition gained interest in the 1940s with the introduction of sulfonamides, a family of synthetic antibiotics that inhibited a bacterial enzyme essential for cell growth, thus underscoring proteins as relevant drug targets\cite{maehle_emergence_2002}.


The remarkable success of antibiotics in the mid-20th century, coupled with the post-WWII economic growth and advancements in organic chemistry and pharmacology, encouraged the pharmaceutical industry to explore novel therapeutic areas. Notably, the introduction of antipsychotics (e.g. chlorpromazine \cite{ban_fifty_2007}), antihistamines (e.g. diphenhydramine \cite{simons_histamine_2011}), antidepressants (e.g. imipramine \cite{brown_clinical_2015}) and beta-blockers (e.g. propranolol \cite{srinivasan_propranolol_2019}) paved the way for significant advances in the treatment of psychiatric disorders, allergies and cardiovascular diseases. Although numerous drugs were still discovered either serendipitously (e.g. being primarily designed to treat other pathologies) or empirically (e.g. testing large collections of compounds for desired biological activity without necessarily understanding the underlying molecular mechanisms), the first examples of designed drugs meant to target specific proteins began to emerge\cite{drews_drug_2000}.

Indeed, the increasing interest in proteins as potential drug targets changed the way in which researchers conceived drug discovery. Living organisms were progressively no longer seen as black boxes but as systems with specific, targetable biochemical entities (e.g. proteins) whose modulation could result in therapeutic effects, laying the foundation for the rational design of new drugs (aka reverse pharmacology). 

\subsection{Rational drug design}
\label{Introduction_rational}

The main hypothesis behind rational drug design is that by modulating the activity of a specific target associated with a disease (e.g. a protein), it is possible to halt or revert the progression of the disease or to alleviate its symptoms. The modulation is usually achieved with an external compound (i.e. a drug) that interacts with the target to exert its therapeutic action, ideally without causing significant side effects.

Of special interest is propranolol, the first clinically successful beta-blocker, developed by Nobel laureate Sir James Black in the early 1960s, and primarily used to treat angina and hypertension\cite{srinivasan_propranolol_2019}. The intended targets of propranolol were the beta-adrenergic receptors, under the hypothesis that blocking the action of adrenaline or noradrenaline in those receptors would decrease blood pressure and heart rate. Indeed, Black based his design on pronethalol, an already known antagonist of the beta-adrenergic receptors, and, by slightly modifying its chemical structure, he managed to increase the potency of the compound and to reduce its carcinogenicity\cite{black_new_1964}. In this way, propranolol was designed with a blended empirical-rational drug design approach.

The 1960s also marked the advent of protein crystallography, leading to a paradigmatic shift in the development of new drugs. Myoglobin and hemoglobin were the first protein structures to be experimentally resolved using X-ray crystallography, a breakthrough for which Kendrew and Perutz were awarded the Nobel Prize in Chemistry back in 1962\cite{kendrew_three-dimensional_1958, muirhead_structure_1963}. Indeed, the availability of protein structures opened the door not only to understanding protein function from a molecular perspective, but also to studying fine binding details between endogenous small molecules (e.g. heme\cite{kendrew_three-dimensional_1958, muirhead_structure_1963}) or drugs (e.g. methotrexate \cite{bolin_crystal_1982}) and their respective targets. This shift provided a comprehensive molecular framework to disrupt or enhance the effects of endogenous ligands as well as to improve the potency of known drugs. Interestingly, the 1990s saw the approval of the first drugs whose design was based on specific structural details of the target protein binding sites, such as saquinavir (targeting the HIV protease to treat HIV infections) and dorzolamide (targeting the carbonic anhydrase to treat eye hypertension)\cite{rondeau_protein_2008, leelananda_computational_2016}. 

Apart from exhibiting sufficient binding affinity towards the intended target, drugs need to be optimized for metabolic stability and minimal toxicity. In addition, and depending on the route of administration, drugs must first be efficiently absorbed into the bloodstream, which requires a delicate balance between aqueous solubility and biological membrane permeability, factors that critically determine its bioavailability. Once in the bloodstream, drugs will diffuse throughout the body to eventually reach its target tissue(s), potentially crossing specific biological barriers (e.g. blood-brain barrier) and ultimately traversing cell membranes when necessary. Furthermore, drugs must be efficiently eliminated from the body to prevent its accumulation and subsequent toxicity. Overall, rational drug design is a multi optimization process in which many variables need to be considered simultaneously, typically requiring an iterative cycle involving computational modeling, chemical synthesis, biological testing and, eventually, clinical evaluation\cite{waring_analysis_2015, patrick_introduction_2023}. 

In the context of rational drug design, the “magic bullet” concept introduced by Ehrlich in the early 20th century, which envisioned the specific target of disease-causing agents, evolved into the more modern but still reductionist “one disease-one gene-one drug” paradigm. Although such an enticing view has led to numerous successful drug approvals, it has inherent limitations \cite{samsdodd_target-based_2005, swinney_how_2011}. Indeed, not all genes (i.e. proteins) are amenable for modulation by a drug-like compound, such as those lacking a defined binding site in their three-dimensional constituting domains. Additionally, the massive release of biological data (i.e. omics) following the sequencing of the human genome in the early 2000s \cite{venter_sequence_2001, international_human_genome_sequencing_consortium_initial_2001, field_omics_2009, mardis_decades_2011} has uncovered an overwhelming complexity of biological systems\cite{kitano_systems_2002}, showing that human disease cannot be fully understood by studying their individual components alone. In fact, complex diseases (e.g. cancer, diabetes) usually result from multiple disruptions within intricate biological networks (e.g. protein-protein interactions), influenced by both genetic and environmental factors, that cannot be pinned down to a single gene malfunction. To address this, emerging fields such as systems pharmacology aim at adopting a more holistic view on biological systems, thus bridging the gap between phenotypic and target-based drug discovery.

In any case, whether drug targets are single proteins or complex biological processes, most approved drugs are still new molecular entities\cite{mullard_2023_2024}. Indeed, small molecules are an excellent tool to probe biological functions and the primary choice of pharmaceutical companies, as they are easy to manufacture, store, and distribute, and synthetic chemists can conceive a broad variety of them.

\subsection{The Chemical Space of small molecules}
\label{Introduction_chemicalspace}

Out of the essentially infinite distinct organic molecules one can dream of, only a portion have suitable properties to be used as drugs and have thus potential for human clinical use. In a prospective work conducted in the late 1990s, Lipinksi and colleagues noted that a significant proportion of small molecules that had entered Phase II clinical trials at that time (2,245 compounds) shared chemical and physical properties that tended to fall within specific ranges (e.g. molecular weight <500 Da)\cite{lipinski_experimental_2001}. This observation led to the formulation of the Lipinski's “Rule of Five”, a set of guidelines to prioritize drug-like compounds in early stages of drug discovery, further enabling the exploration of the chemical space (i.e. the infinite set of organic molecules) in a manner relevant to drug development\cite{lipinski_navigating_2004, dobson_chemical_2004, reymond_chemical_2010}. Some studies estimate the number of drug-like small molecules to be in the range of 10\textsuperscript{33}-10\textsuperscript{60}\cite{bohacek_art_1996, polishchuk_estimation_2013}. However, and despite the huge amount of potentially bioactive compounds that exist within the chemical space, the number of approved small molecule drugs is comparatively very small, standing at 2,781 as of March 2024 (DrugBank\cite{knox_drugbank_2024} v 5.1.12). This stark contrast underscores the significant cost and complexity associated with drug development efforts, which on average take billions of US\$ and more than a decade to bring a new drug to the market\cite{wouters_estimated_2020, sertkaya_costs_2024, dimasi_innovation_2016, hinkson_accelerating_2020, dimasi_research_2020}, primarily due to high attrition rates in clinical trials\cite{sun_why_2022}. 

Before biological tests are performed to evaluate the therapeutic potential of a candidate compound, it first has to be synthesized in sufficient amounts. The size of commercial (e.g. Enamine REAL Space) and proprietary libraries (e.g. Merck MASSIV 2018) has dramatically increased in the last decade, including up to 10\textsuperscript{9} and 10\textsuperscript{20} accessible chemical compounds, respectively, with synthetic routes success rates that may exceed 80\%\cite{stein_virtual_2020, hoffmann_next_2019, klingler_sar_2019}. This is mainly due to the implementation of strategies based on two- or three-step three-component reaction sequences and the availability of starting materials with pre-validated chemical reactivity \cite{grygorenko_generating_2020}. In addition, high-throughput screening (HTS) assays have penetrated the public research sector in recent years, providing depth of biological annotation to compound collections\cite{subramanian_next_2017, corsello_discovering_2020}. This is reflected in the increasing number of bioactive small molecules cataloged in open databases, which already amount to over two million entries\cite{gaulton_chembl_2017, wang_pubchem_2017}. Additionally, the effects of small molecules in biological systems are multifaceted and often annotated through various levels of biological complexity. For instance, a compound may bind specific biochemical targets, disrupt or modulate certain signaling pathways, rewire protein interaction networks, induce changes in gene expression, modify protein abundance or alter cell morphology.

As the number of available compounds and associated biological data continues to grow, the need for effective methods to manage, analyze, and interpret this vast amount of information has become increasingly important. This demand has spurred the development of fields such as cheminformatics and bioinformatics, which aim to increase the efficiency of drug discovery processes. 


\subsection{Cheminformatics and the similarity principle}
\label{Introduction_cheminformatics}

Blending chemical knowledge, biological data and computer science in the context of drug discovery has been a cornerstone for the cheminformatics field since the advent of computation \cite{brown_chemoinformatics_1998, engel_basic_2006}. Small molecules are usually depicted in the form of SMILES, a string-based molecular representation that, although ambiguous (i.e. a single molecule may be described with multiple SMILES), is very intuitive and has been broadly adopted in the field\cite{weininger_smiles_1988}. In addition, the International Chemical Identifier (InChI) and its hashed version, the InChIKey, provide a more standardized and unambiguous way to represent molecules, ensuring consistent identification across databases \cite{heller_inchi_2015}.

Cheminformatics is founded on the small molecule similarity principle, which states that structurally similar compounds are likely to have similar properties (e.g. biological activities) \cite{johnson_concepts_1990}. The principle has long guided medicinal chemistry efforts, where it is used to predict and optimize the biological activity of new compounds. Paradoxically, medicinal chemists often seek slight structural changes that lead to optimal bioactivity and minimal toxicity for a given compound (e.g. activity cliffs). In fact, such changes in bioactivity may be attributable to subtle variations in the interactions with the corresponding binding partner (e.g. a protein) or to the inherent complexity of biological systems (e.g. the compound is not metabolically stable). 

Historically, the biological testing of structurally related compounds has significantly contributed to the discovery of many drugs, such as Ehrlich’s arsphenamine (a chemical derivative of the highly toxic atoxyl) and Sir James Black’s propranolol (a structural analog of the carcinogenic pronethalol), both mentioned previously. A particularly relevant example is the case of benzodiazepines (Fig \ref{Introduction_Fig1}), a class of depressant drugs used to treat psychiatric conditions such as anxiety disorders and insomnia. Chlordiazepoxide (1960), the first marketed benzodiazepine, paved the way for the development of diazepam (1963), oxazepam (1965), lorazepam (1977) and alprazolam (1981). The mentioned drugs share a common 1,4-benzodiazepine skeleton bound to an aromatic ring and an electron-withdrawing chlorine atom, making them highly similar from a structural perspective\cite{wick_history_2013, costa_clinical_1984}. Indeed, as noted by Black himself, “the most fruitful basis for the discovery of a new drug is to start with an old drug”\cite{pillaiyar_medicinal_2020, raju_nobel_2000}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/Introduction/Benzodiazepines.png}
  \caption{
    \textbf{The small molecule similarity principle.} 
    Chemical structures of chlordiazepoxide, diazepam, oxazepam, lorazepam and alprazolam, approved for medical use in 1960, 1963, 1964, 1977 and 1981, respectively. Yellow areas indicate shared scaffolds and substructures across them.
  }
  \rule[0ex]{\textwidth}{0.5pt}
  \vspace{-5mm}
  \label{Introduction_Fig1}
\end{figure}

Although the quantitative evaluation of small molecule similarity is the holy grail of cheminformatics, other tasks involve the comprehensive visualization of the chemical space or the numerical correlation between chemical structures or molecular features and a property of interest (i.e. QSAR\cite{lenselink_beyond_2017}). In any of those cases, small molecules need to be characterized in a format amenable to computational applications. 


\subsection{Small molecule chemical descriptors }
\label{Introduction_chemicaldescriptors}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/Introduction/figure1_COCB.png}
  \caption{
    \textbf{Small molecule descriptors --encoding compounds through their chemistry and bioactivity.} 
     Classical small molecule descriptors may include global physicochemical properties (0D, e.g. molecular weight), the presence of specific structural features (1D, e.g. ECFPs), molecular topology (2D, e.g. graphs), molecular conformation (3D, e.g. atom cartesian coordinates) or environment-dependent properties (4D, e.g. interactions with the protein target). Data-driven descriptors may be purely chemical, usually through the use of autoencoders, or encapsulate bioactivity information such as protein binding, induced gene expression changes or clinical outcomes.
  }
  \rule[0ex]{\textwidth}{0.5pt}
  \vspace{-5mm}
  \label{Introduction_Fig2}
\end{figure}

In practice, representing chemical compounds in a meaningful way for cheminformatics-related tasks requires the selection of a small molecule numerical descriptor. Molecular descriptors allow for the mathematical treatment of chemical and structural features of molecules and facilitate their subsequent use in computational applications. 


Various strategies exist to generate such descriptors (Fig \ref{Introduction_Fig2}). The simplest approaches account for global molecular properties (0D, e.g. molecular weight) or the presence of particular structural features (1D). Of particular interest are the broadly used extended-connectivity fingerprints (ECFPs \cite{rogers_extended-connectivity_2010}, encoding the circular environment of each atom up to a specific radius) and the molecular access system keys (MACCS Keys \cite{durant_reoptimization_2002}, accounting for the presence or absence of a specific set of pre-defined chemical groups), both being 1D small molecule fingerprints. Additionally, the molecular topology (2D, e.g. distance matrices between atoms) or the spatial information of the atoms (3D, e.g. cartesian coordinates) can be encapsulated by conveniently representing molecules as chemical graphs (atoms being nodes and bonds being edges) \cite{devinyak_3d-morse_2014}. Finally, further sophisticated methods capture environment-dependent properties, such as functional regions or intramolecular interactions (4D, e.g. energetically favorable binding sites or multiple MD-derived conformational states)\cite{pastor_grid-independent_2000, riniker_molecular_2017}. Although representing compounds with relatively complex frameworks (i.e. graphs) may provide a more natural depiction of molecules, simple 1D fingerprints offer numerous advantages, particularly in terms of computational efficiency, search speed, simplicity and interpretability \cite{rogers_extended-connectivity_2010}. In summary, these chemical descriptors are at the core of chemoinformatics and are still the first choice in most applications\cite{david_molecular_2020, cereto-massague_molecular_2015}.

However, the last years have witnessed the expansion of a new generation of molecular descriptors (Fig \ref{Introduction_Fig2}). Such descriptors, deemed to be data-driven and based on deep learning approaches, are engineered on the basis of large-scale chemistry databases and are thus adaptable to a given task or region of the chemical space of small molecules\cite{sanchez-lengeling_inverse_2018}. In particular, graph and text-based autoencoders\cite{hinton_reducing_2006, baldi_autoencoders_2012} are able to embed the information provided by 2D molecule structures and SMILES strings, respectively, into a dense numerical vector belonging to a “latent space” \cite{jin_hierarchical_2019, gomez-bombarelli_automatic_2018, liu_constrained_2018, polykovskiy_molecular_2020}. Simple measures such as Euclidean distances within the latent space are able to capture chemical similarity and, when coupled to machine learning algorithms, these descriptors have shown state of-the-art performance in several biophysics and physiological benchmark datasets\cite{wu_moleculenet_2018}.

\subsection{Extending the similarity principle beyond chemical structures}
\label{Introduction_extending}

A natural extension of the first generation of data-driven chemical descriptors is to include the wealth of bioactivity information available in the databases, to encapsulate, in the form of bioactivity descriptors, the experimental evidence gathered over years of research (Fig \ref{Introduction_Fig2}). Indeed, the use of bioactivity signatures enable the exploration of the drug-like chemical space in a biologically relevant manner. 

In pioneering work, instead of focusing on chemical structures, Kauvar et al. \cite{kauvar_predicting_1995} characterized a set of compounds according to their ability to bind a panel of 18 receptors and used these affinity profiles to assess similarities between them. The idea of relating small molecules based on their target profiles was further developed over the next years\cite{paolini_global_2006, keiser_predicting_2009}, enhancing the performance in classical chemoinformatics tasks (e.g. target prediction). In a more complex attempt to capture phenotypic effects induced by drug activity in cells, MacDonald et al. \cite{macdonald_identifying_2006} used a protein complementation assay to monitor the status of several cellular pathways after compound perturbation. Then, they derived pathway activity fingerprints for over a hundred compounds and found that pathway-based similarities strongly correlated with known structure-activity relationships. Similarly, Young et al. \cite{young_integrating_2008} combined automated microscopy with image analysis to profile the biological effects of a compound library. They integrated the resulting phenotypic profiles with the chemical structure of the compounds and their predicted targets and found that the combination of the three features had a substantially higher capacity to identify mechanisms of action than either one of them alone.

Indeed, the popularity of HTS assays has revealed that it is possible to establish relationships between compounds based on their functional activity rather than their chemical structure. For instance, it was suggested that molecules triggering similar transcriptional responses in cell lines might share mechanisms of action, an observation that inspired the implementation of the connectivity map \cite{lamb_connectivity_2006} and the following library of integrated network-based cellular signatures (LINCS L1000) \cite{subramanian_next_2017} initiatives. These libraries provide a catalog of transcriptional signatures in different cell lines, measured as a result of a systematic screening of genetic (CRISPR or shRNA) and pharmacological perturbations, which has been exploited, for instance, to suggest potential targets for a given compound\cite{sawada_predicting_2018}. Likewise, molecules that inhibit the growth of a similar subset of cell lines (i.e. that have similar sensitivity profiles) \cite{holbeck_analysis_2010} or drugs that elicit similar side effects, also tend to share mechanisms of action \cite{campillos_drug_2008}, even if their chemical structures appear to be unrelated.

Building upon these seminal works, our lab recently presented the Chemical Checker (CC), a resource that integrates the major chemogenomics and drug activity repositories and represents the largest collection of small molecule bioactivity signatures available to date \cite{duran-frigola_extending_2020}. The CC gathers experimentally determined bioactivity data for about 1M small molecules in the medicinal chemistry space and provides bioactivity descriptors in five levels of increasing biological complexity. The first level of descriptors characterizes the chemical properties of the compounds, including their 2D and 3D structures, scaffolds, functional groups, and physicochemical properties. The second level captures information on the protein receptors of the molecules, including known mechanisms of action, metabolizing enzymes and HTS binding assays. Descriptors in the third level of complexity address the propagation of the target perturbations triggered by the small molecules, including protein-protein interactions and pathways provided by several types of biological networks. The fourth level of signatures captures the bioactivity of the compounds measured at the cellular level, with assays including differential gene expression and sensitivity profiles in cancer cell-line panels. Finally, for the few compounds that reached clinical stages, the fifth level of CC signatures encodes details on their therapeutic areas, adverse side effects and drug-drug interactions.

A known limitation of the CC is that the number of molecules with reported bioactivities diminished at each level of complexity, and thus, we could only derive a limited set of bioactivity descriptors corresponding to a minority of well-characterized compounds. To extend the coverage of bioactivity descriptors to uncharacterized molecules, we trained a collection of deep neural networks (i.e. ‘signaturizers’) that are able to infer bioactivity signatures for any compound of interest, even when only its chemical structure is available. We were able to assign a confidence score to the predictions of the signaturizers and systematically apply them to sets of compounds beyond drug molecules, including plant metabolites and food ingredients\cite{bertoni_bioactivity_2021}.

Overall, bioactivity signatures provide a complementary means to describe small molecules, focusing on the integration of multiple types of experimental data \cite{wassermann_opportunities_2015}. Indeed, these descriptors have proven useful to navigate the chemical space in a biologically relevant manner and boost the performance in many drug discovery tasks that typically rely on chemical descriptors, such as target identification and toxicity prediction\cite{bertoni_bioactivity_2021}.

\subsection{Bioactivity in three dimensions}
\label{Introduction_bioactivity}

In the quest to leverage experimental data for generating bioactivity descriptors, chemical compounds represent only one part of the equation. These compounds (e.g. drugs) must physically interact with the disease-related target (e.g. a protein) to modulate its activity and exert the corresponding therapeutic effect, as assumed under the reverse pharmacology paradigm\cite{takenaka_classical_2001, hacker_pharmacology_2009}. Protein-ligand physicochemical interactions are three-dimensional, fully dependent on the chemical and structural complementarity between both entities\cite{kitchen_docking_2004, mcgovern_specific_2003}. 

Stereoisomerism refers to the type of isomerism where chemical compounds have identical molecular formulas and atom connectivities but differ in the three-dimensional spatial arrangement of their atoms \cite{gold_iupac_2019}. Since biological matter is intrinsically chiral (i.e. distinguishable from its mirror image), stereoisomeric small molecules may be differently absorbed, distributed, metabolized and excreted (i.e. pharmacokinetics) as well as exhibit completely distinct protein binding profiles (and thus therapeutic effects, i.e. pharmacodynamics) \cite{inaki_cell_2016, mcconathy_stereochemistry_2003}. While pharmacological differences between stereoisomers are commonly expected in the case of diastereomers (stereoisomers that are not mirror images and exhibit distinct chemical properties, e.g. cisplatin and transplatin \cite{dasari_cisplatin_2014, trudu_coordination_2015}), such differences may not be as apparent in the case of enantiomers. Indeed, enantiomers are non-superimposable mirror images of each other whose separation and purification are usually challenging due to their identical physicochemical properties in non-chiral environments\cite{ward_chiral_2012}. An illustrative example is the case of (S)-Penicillamine, an antiarthritic drug administered as an enantiomerically pure compound due to the high toxicity of (R)-Penicillamine, its enantiomeric counterpart\cite{aronson_meylers_2010}. 

The CC offers an unprecedented opportunity to explore large scale bioactivity data in the context of stereoisomerism, quantifying to which extent stereoisomer pairs exhibit distinct bioactivities (e.g. protein binding profiles). In addition, and although the relevance of stereoisomerism in pharmacological applications is well known, the enumeration of all possible stereoisomers given a parent compound is usually missed when creating large chemical libraries in favor of maximizing chemical diversity (i.e. covering a broad area of the chemical space)\cite{ruddigkeit_enumeration_2012}. In line with this, original signaturizers to infer bioactivity signatures for uncharacterized compounds were built on 2D molecular representations (ECFPs) and are thus unable to capture differences between stereoisomer pairs (i.e. the inferred bioactivity signatures are identical). There is therefore a clear need to bridge the gap between small molecule three-dimensional conformations and bioactivity signatures by developing a new generation of stereochemically-aware signaturizers.

\subsection{Protein structures}
\label{Introduction_proteinstructures}

Just as the spatial orientation of atoms in small molecules is crucial to their properties, the three-dimensional structure of proteins is fundamental to their role in biochemical processes. Proteins, which are linear sequences of amino acids resulting from the transcription of DNA and the following translation of mRNA, typically fold into specific three-dimensional shapes. The folding event is driven by a set of intramolecular interactions governed by thermodynamic principles, though the exact mechanisms underlying this process are not yet fully understood (i.e. the protein folding problem) \cite{anfinsen_principles_1973}. In Structural Biology, the central dogma posits that a protein's sequence determines its structure, and its structure, in turn, dictates its function \cite{maynard_smith_natural_1970, aharoni_evolvability_2005, redfern_exploring_2008}.

Indeed, proteins associated with a given disease are usually the main target in classical small molecule-based drug discovery efforts \cite{hughes_principles_2011, overington_how_2006, imming_drugs_2006}. The availability of the first solved protein structures in the 1960s, determined through X-ray crystallography, was a turning point in the field, providing a rational framework for designing candidate drugs on a molecular basis. Although the structural characterization of proteins was a tedious process that required their experimental crystallization and the analysis of X-ray diffraction patterns, the number of released structures increased exponentially since the 1970s. The Protein Data Bank (PDB)\cite{berman_protein_2000}, the primary repository of freely available experimentally determined protein structures created in 1971, contained about a hundred structures in the early 1980s, which grew up to 100k by 2014 and surpassed 200k by 2023\cite{burley_rcsb_2023}.

Despite the steady increase in the number of resolved protein structures, a significant gap remained between the known protein sequences (29M by 2014 and 227M by 2023 in the UniProtKB) and their experimentally determined structures\cite{the_uniprot_consortium_uniprot_2023}. To bridge this long-standing gap, homology modeling emerged in the 1990s as a computational approach that provided valuable insights into proteins lacking structural characterization, provided that the structure of a homologous protein with sufficient sequence identity was already available \cite{marti-renom_comparative_2000, schwede_swiss-model_2003}. In parallel, other computational strategies were further designed to predict protein structures \textit{ab initio}, most of them based on the laws of thermodynamics without the need of a template structure \cite{bertoline_before_2023}. The Critical Assessment of Structure Prediction (CASP), a community challenge first conducted in 1994, was created to evaluate the accuracy of protein structure prediction methods to correctly model the structures of proteins that had not yet been publicly released\cite{moult_largescale_1995}. Although minor improvements were made over the years, no massive breakthrough was achieved during the next couple of decades, and the results of computational predictions were still far from exhaustively matching experimentally determined protein structures\cite{moult_critical_2018}.

The wealth of protein structural data accumulated in the 2010s, coupled with the successful incorporation of evolutionary data (i.e. two protein residues that coevolve are likely to be close in space) and deep learning algorithms (i.e. neural networks with multiple hidden layers) set the stage for a long-awaited paradigm shift in the field of structure prediction. AlphaFold2 (AF2), developed by DeepMind, introduced a transformer-based architecture built on attention mechanisms that integrated evolutionary and structural data, along with an iterative refinement process to generate predicted structures that were, in most cases, near to experimental accuracy\cite{jumper_highly_2021}. DeepMind thus leveraged decades of invaluable experimental knowledge with a revolutionary approach to effectively address, and solve, the protein structure prediction problem (not to be confused with the protein folding problem), as demonstrated in the CASP14 assessment conducted in 2020\cite{pereira_highaccuracy_2021}. AlphaFold2 has indeed proven transformative since its public release in 2021, providing structural biologists accurate protein models at an unprecedented scale\cite{tunyasuvunakool_highly_2021, bryant_improved_2022, wong_benchmarking_2022}.  

However, proteins are dynamic entities rather than static structures, and exploring their conformational landscape is essential for a comprehensive understanding of their biological roles \cite{henzler-wildman_dynamic_2007, motlagh_ensemble_2014}. Indeed, both AlphaFold2-predicted models and experimentally-resolved X-ray structures are static snapshots that capture a single conformation among the many a protein may adopt under physiological conditions. Nuclear Magnetic Resonance (NMR) approaches partially overcome this limitation, enabling researchers to observe proteins in solution and providing insights into their intrinsic flexibility and the conformational changes they may undergo \cite{kleckner_introduction_2011}. In addition, proteins are not isolated entities within the cellular environment. Rather, they interact with a plethora of endogenous biomolecules, including other proteins, nucleic acids, lipids, ions and small metabolites. Such interactions, as well as post-translational modifications (e.g. phosphorylation), can induce significant conformational changes, affecting their structure and activity. The growing interest in understanding proteins within their biological context has led to advancements in techniques like cryo-electron microscopy (cryo-EM), which allows the visualization of single proteins and larger complexes in near-native states without the need for crystallization \cite{kuhlbrandt_resolution_2014, renaud_cryo-em_2018, bai_how_2015}.

In parallel to the increasing availability of protein structural data, computational approaches have been developed and refined to exploit these data \cite{pagadala_software_2017, hollingsworth_molecular_2018}. Molecular and macromolecular docking enable researchers to predict how proteins interact with other molecules, including potential therapeutics like small-molecule drugs, and provide a rational framework to model, analyze and optimize intermolecular interactions. Complementary to this, molecular dynamics (MD) simulations offer insights into the temporal behavior of proteins, allowing the exploration of their conformational landscape and their responses to external factors over time. 

Although these computational methods are invaluable, their impact on drug discovery efforts remains limited due to multiple factors. Prevailing challenges include the absence of a universal scoring function that can accurately predict binding affinities across different systems \cite{li_overview_2019, shen_machine_2020}, the high computational cost associated with MD simulations \cite{hollingsworth_molecular_2018}, and the inherent complexity of biological systems that defies the simplistic "one disease-one gene-one drug" paradigm.

\subsection{Protein and binding site descriptors}
\label{Introduction_proteindescriptors}

To match the rich chemical representations of small molecules, researchers are also developing computational methods to encapsulate information available for the target proteins (Fig \ref{Introduction_Fig3}). Protein sequence descriptors, for example, annotate the identity and the physicochemical properties of each amino-acid \cite{hellberg_peptide_1987} or measure general features of the full length sequence, such as global residue composition and distribution \cite{xiao_protrprotrweb_2015}. In any case, these relatively simple representations have been used in a battery of bioinformatics tasks, including protein engineering \cite{xu_deep_2020} and function prediction \cite{kulmanov_deepgoplus_2020}. Like in the case of data-driven descriptors for small molecules, deep learning is providing new ways to describe biological sequences. For instance, Alley et al. \cite{alley_unified_2019} applied deep neural networks to a vast set of unlabeled sequences, yielding semantics rich descriptors that captured structural, evolutionary and biophysical properties of proteins. These descriptors have proven their value to predict the stability of \textit{de novo} designed proteins, but their agnostic nature and versatile format make them a suitable input for almost any machine learning task involving proteins. In general, protein sequences are treated as text data, which allows for borrowing techniques from natural language processing, a discipline that has made extraordinary progress for knowledge representation \cite{asgari_continuous_2015, heinzinger_modeling_2019}. In a first attempt to systematically benchmark language models (LMs) for protein modeling, Rao et al. \cite{rao_evaluating_2019} designed a set of tasks assessing protein embeddings and reported promising results for a variety of models involving evolutionary understanding and protein engineering. Additionally, while reviewing the new advances in language modeling for protein sequences, Bepler and Berger \cite{bepler_learning_2019} trained a protein LM conditioned to structure prediction tasks (i.e. the model was forced to predict residue contacts and structural similarity during training) \cite{bepler_learning_2021}. By including evolutionary and structural information, they not only showed improvements in downstream tasks (e.g. protein function prediction) but also evidenced that hybrid approaches leveraging both data-driven sequences and physics-based domains can help LM to better embrace the sequence-structure function paradigm. However, similar to any method involving deep learning, the interpretability of these protein LMs is quite limited. In a remarkable attempt to shed light on the biological and biophysical information captured by transformer-based descriptors, Vig et al. \cite{vig_bertology_2020} thoroughly analyzed the inner layers of the deep neural network and found that they uncovered relevant associations in the three-dimensional space, such as residues that were far apart in the sequence but spatially close in the structure as well as those constituting the protein binding sites. 


\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/Introduction/figure2_COCB.png}
  \caption{
    \textbf{Protein descriptors --encoding proteins through their sequence and structure.} 
     Classical sequence-based protein descriptors may include the identity or the physicochemical properties of their amino acids, either individually (i.e. one-hot encoding) or using sliding windows to capture their short-range environment. To account for more distant amino-acid relationships, proteins can be encoded using techniques borrowed from natural language processing (i.e. word embeddings or attention models), where sequences are often treated as a set of constant-length overlapping fragments or k-mers. Whenever high-resolution models of target proteins are available, these can be used to derive structure-based descriptors. The classical ones consider the geometry and physicochemical properties of the binding pockets by calculating distances between pharmacophoric points and transforming them into high–dimensional profiles, accounting for the presence or absence of a given pharmacophoric trait. More recently, computer vision and deep learning techniques have been adapted to embed structural properties of protein surfaces and specific binding pocket features.
  }
  \rule[0ex]{\textwidth}{0.5pt}
  \vspace{-5mm}
  \label{Introduction_Fig3}
\end{figure}


Thus, whenever high-resolution structures of the target proteins are available, more specific structure-based descriptors can be developed, usually focusing on interesting regions such as ligand binding sites. Classic pocket descriptors measure their geometrical and electrostatic features and translate them into binary fingerprints that simply account for the presence or absence of a given structural motif, in the same way that ECFPs or MACCS Keys do for chemical compounds\cite{weill_alignment-free_2010, siragusa_biogps_2015, rogers_extended-connectivity_2010, durant_reoptimization_2002}. Cavity similarities based on these binding pocket fingerprints have unveiled interesting cases of remote homology between proteins \cite{stark_model_2003} and are the basis for several polypharmacology strategies\cite{duran-frigola_detecting_2017, chaudhari_up--date_2020}. The popularity of methods to compare druggable pockets prompted the creation of thorough benchmark datasets, such as TOUGH-M1\cite{govindaraj_comparative_2018} and the ProSPECCTs Datasets \cite{ehrt_benchmark_2018}, which pointed out the strengths and weaknesses of a variety of descriptor types and approaches, and provided a gold standard to validate pocket comparison strategies to come. Systematic evaluation has revealed that some descriptors are better suited to compare active sites of related proteins, while others perform better to describe macromolecular binding interfaces, being the latter more appropriate for drug polypharmacology and repurposing studies\cite{ehrt_binding_2019}.




In addition, if progress in natural language processing has enabled data-driven sequence-based protein descriptors, progress in image analysis and computer vision has prompted the development of three-dimensional structure-based descriptors. For instance, Gainza et al. \cite{gainza_deciphering_2020} devised a novel strategy to segment high-resolution protein surfaces into overlapping radial patches, mapping chemical and geometrical features onto them. These data are then transferred into a convolutional neural network (CNN) to generate the descriptors, which can be finetuned for specific tasks, such as ligand-binding pocket similarity or protein-protein interaction interface comparisons. DeeplyTough is another recent method that also uses CNNs to encode three-dimensional characteristics of protein binding pockets\cite{simonovsky_deeplytough_2020}. The peculiarity of DeeplyTough is that it has been trained to ensure that similar pockets are encoded into similar descriptors, while retaining the ability to account for small structural variations and differentiate closely related binding sites.

However, existing methods for characterizing protein binding sites exhibit several intrinsic limitations. For instance, some strategies require the explicit presence of a bound ligand to effectively identify the most relevant interactions occurring within the binding site, limiting their applicability domain to \textit{holo} structures \cite{wood_pharmacophore_2012, desaphy_encoding_2013}. Additionally, most methods rely on hand-crafted pocket representations, often selecting parameters derived from biased datasets, which results in poor performance in broader, more realistic scenarios \cite{ehrt_benchmark_2018}. Moreover, several strategies rely on alignment-dependent comparisons, which can provide significant insights into the underlying patterns of binding site similarity, but also come along with increased computational cost \cite{schalon_simple_2008}. Finally, those strategies based on deep learning suffer from lack of interpretability, a well-known problem in the field\cite{jimenez-luna_artificial_2021, vamathevan_applications_2019, ching_opportunities_2018}.

Given the promise of structure-based protein pocket descriptors, the increasing availability of protein structures (both experimentally-determined and computationally-predicted) and the inherent limitations of existing strategies to characterize druggable pockets, there is a clear need to develop a new methodology to generate novel pocket descriptors. This new approach should overcome the mentioned limitations while enabling the exhaustive exploration of complete proteomes from the pocket perspective.  


\subsection{This Thesis into context}
\label{Introduction_context}

Small molecules and proteins are fundamental actors in the intricate processes taking place within human cells. The avalanche of biochemical data collected in the last decades, the remarkable progress in organic synthesis and the rapidly increasing computational power, has given both actors a fundamental role in experimental and computational drug discovery efforts. In fact, both small molecules and proteins are definite sets of bound atoms in space, but their computational treatment often requires simplified representations amenable to efficient comparison and manipulation. 

All the work presented here orbits around the concept of numerical descriptors to represent and characterize molecules. Regarding proteins, our efforts focus on druggable binding sites, specific protein surface regions that interact with smaller molecules and thus play a determinant role in the biological functions developed by proteins. It is the main goal of this thesis to further expand the concept of small molecule and protein binding site descriptors, addressing the main limitations of existing methods and exploring new approaches to enhance their ability to capture relevant structural, chemical and functional features. 

It is important to mention that this work is not focused on a specific small molecule, drug, protein, binding site or disease, but rather introduces methodological frameworks to process novel bioactivity data and characterize biochemical entities in the shape of numerical descriptors. In addition, both small molecule and pocket descriptors are used to explore and visualize the chemical and pocket spaces, respectively, in a comprehensive manner. 

The thesis is organized in four different chapters and summarized in Fig \ref{Introduction_Fig4}. Each chapter is meant to be fully independent and self-explanatory. This is why the individual chapter introductions may partially overlap with this more general and broad section. 

\begin{Figure_modified}
  \centering
  \makebox[\textwidth]{
  \includegraphics[scale=1.3]{figures/Introduction/ThesisScheme_v3.png}
  % \includegraphics[width=\linewidth]{figures/Introduction/ThesisScheme_v3.png}
  }
  \vspace{-15mm}
  \caption{
    \textbf{Thesis contextualization.} 
    The realm of Science covers a broad spectrum: from tiny atomic details to the vast expanses of galaxies. In between, interactions among small molecules and macromolecules take place and form the basis of complex systems that may eventually culminate in life. This thesis is focused on the numerical characterization of small molecules and proteins (druggable pockets), a common approach in the field of Cheminformatics and Structure-based Drug Discovery. In the first chapter (3.1), we use a predefined computational framework (the Chemical Checker) to process, harmonize and integrate publicly available bioactivity data, resulting in novel bioactivity signatures. In the second chapter (3.2), we cluster the chemical space of small molecules and explore it in a biologically relevant manner, showcasing how bioactivity signatures may anticipate relationships between compounds that can not be predicted from their chemical structures alone. Next (chapter 3.3), we evaluate the link between stereoisomerism and bioactivity and train a new generation of signaturizers that provide stereochemically-aware bioactivity signatures. Finally, in the last chapter (3.4) we present a novel methodology to generate numerical descriptors for protein druggable pockets and show its ability to capture similarities at pocket level between human protein domains.
  }
  \vspace{-5mm}
  \rule[0ex]{\textwidth}{0.5pt}
  \vspace{-9mm}
  \label{Introduction_Fig4}
\end{Figure_modified}


In the first chapter (3.1), we implement a predefined computational framework to process, harmonize and integrate experimental data (e.g. transcriptomics and proteomics) to build novel bioactivity signatures. In the second chapter (3.2), we use chemical and bioactivity signatures (generated with the signaturizers, a collection of deep neural networks previously developed in the lab) to cluster the chemical space of small molecules and explore it in a biologically relevant manner. In addition, we also illustrate the advent of bioactivity signatures, unraveling similarities between small molecules that can not be anticipated by chemical structure (chemical signatures) alone. The third chapter (3.3) addresses one of the main limitations of existing signaturizers: the explicit consideration of molecular stereochemistry. Apart from conducting a large-scale evaluation of the relationship between stereoisomerism and bioactivity, we develop a second generation of signaturizers, now able to capture subtle differences among stereoisomers. After the systematic navigation through the cheminformatics field (from the raw bioactivity data digestion to the improvement of current signaturization strategies) we move to the second part of the equation in the context of drug action (i.e. the macromolecular targets). The culminating and most extensive chapter of this thesis (3.4) focuses on the design of a novel methodology to represent protein binding sites in the form of pocket descriptors. The approach, which overcomes the main limitations of existing pocket characterization methods, is exhaustively benchmarked and used to comprehensively characterize the set of pockets found in human protein domains. Although this approach is concentrated in the final chapter, it represents the majority of the effort and work undertaken in this thesis.

\renewcommand{\thesubsection}{\thechapter.\arabic{section}.\arabic{subsection}}
